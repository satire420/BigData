# -*- coding: utf-8 -*-
"""BraTS2020 Brain Tumor Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zi0f7GcRM9U8r8GKKyqP9Y6SvP1LvwUa

# BraTS2020 Brain Tumor Segmentation

Takes data from https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation, which is from the BraTS2020 Competition. <br><br> There are 4 goals of the project:
1. Manual segmentation labels of tumor sub-regions,
2. Clinical data of overall survival,
3. Clinical evaluation of progression status,
4. Uncertainty estimation for the predicted tumor sub-regions.

This file is for goal #1: Brain tumor segmentation utilizing the MRI data.

## 1. Download Dataset
"""

#! pip install kaggle

# Set your working directory first so the data downloads where you want
#! kaggle datasets download awsaf49/brats20-dataset-training-validation

#! unzip brats20-dataset-training-validation.zip

"""## 2. Load Data

### Setup Imports
"""

pip install monai

pip install nibabel

pip install onnxruntime

import monai
import os
import torch
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pickle
import shutil
import tempfile
import time
import onnxruntime
import random
import cv2
import PIL
from PIL import Image, ImageOps
import nibabel as nib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix
from skimage import data
from skimage.util import montage
import skimage.transform as skTrans
from skimage.transform import rotate, resize
import glob
from tqdm import tqdm

from monai.apps import DecathlonDataset
from monai.config import print_config
from monai.data import DataLoader, decollate_batch, Dataset
from monai.handlers.utils import from_engine
from monai.losses import DiceLoss
from monai.inferers import sliding_window_inference
from monai.metrics import DiceMetric, compute_confusion_matrix_metric
from monai.networks.nets import SegResNet, UNet
from monai.utils import set_determinism
from monai.transforms import (
    Activations,
    Activationsd,
    AsDiscrete,
    AsDiscreted,
    Compose,
    Invertd,
    LoadImaged,
    MapTransform,
    NormalizeIntensityd,
    Orientationd,
    RandFlipd,
    RandScaleIntensityd,
    RandShiftIntensityd,
    RandSpatialCropd,
    RandRotate90d,
    RandZoomd,
    Resized,
    Spacingd,
    EnsureTyped,
    EnsureChannelFirstd,
    ScaleIntensityd,
)


print_config()

"""### Setup Environment and Directory"""

os.environ["MONAI_DATA_DIRECTORY"] = os.path.expanduser('~/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Individual Work/Deep Learning/MONAI/Brain Tumor')
directory = os.environ.get("MONAI_DATA_DIRECTORY")
if directory is not None:
    os.makedirs(directory, exist_ok=True)
root_dir = tempfile.mkdtemp() if directory is None else directory
print(root_dir)
# List all directories in the given path
folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]

# Print the folders
print(folders)

"""### Rename files that are incorrectly named
The segmentation file in "BraTS20_Training_355" folder has an incorrect name. Before moving forward, rename it to maintain similarity.
"""

# Set path to 355 folder for renaming
rename_PATH = root_dir + "/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/"
print(rename_PATH)

old_name = rename_PATH + "W39_1998.09.19_Segm.nii"
new_name = rename_PATH + "BraTS20_Training_355_seg.nii"

try:
    os.rename(old_name, new_name)
    print("File has been re-named successfully!")
except:
    print("File is already renamed!")

"""## 3. Data Description

The Brain Tumor Segmentation (BraTS) 2020 dataset is a comprehensive collection of MRI scans designed for brain tumor segmentation tasks, specifically targeting glioma cases. For each patient, the dataset provides four different MRI sequences, or modalities, which offer distinct perspectives of the same brain region:

<b>T1-weighted (T1):</b> Standard anatomical imaging <br>
<b>T1 contrast-enhanced (T1ce):</b> Highlights contrast-enhanced regions<br>
<b>T2-weighted (T2):</b> Focuses on water content, useful for visualizing edema<br>
<b>T2-FLAIR (Fluid Attenuated Inversion Recovery):</b> Suppresses fluids, providing clearer visualization of lesions<br><br>
The dataset is also paired with expert-annotated segmentation masks that divide the tumor into different sub-regions:<br>

<b>Label 0:</b> Healthy tissue or background<br>
<b>Label 1:</b> Necrotic and non-enhancing tumor core (NCR/NET)<br>
<b>Label 2:</b> Peritumoral edema (ED)<br>
<b>Label 3:</b> Unused label (no pixels are associated with this class)<br>
<b>Label 4:</b> GD-enhancing tumor (ET)<br><br>
Since Label 3 is unused across the dataset, it is common practice to remap it to Label 4 to maintain label continuity and avoid gaps in the segmentation process.

## 4. Explore Data
Now that the files are named correctly, we can explore the data. <br><br> Let's take a look at patient 147 as an example.
"""

# # load .nii file as a numpy array
  # test_PATH = root_dir + "/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/"
  # test_image_flair = nib.load(test_PATH + "BraTS20_Training_147/BraTS20_Training_147_flair.nii").get_fdata()
  # print("Shape: ", test_image_flair.shape)
  # print("Dtype: ", test_image_flair.dtype)

# print("Min: ", test_image_flair.min())
# print("Max: ", test_image_flair.max())



"""#### Scale Data for Viewing
The max is a little high for our liking, so let's scale the data, just using minmax
"""

# scaler = MinMaxScaler()

# # Scale the image and then reshape it back to original dimensions
# # Keeps spatial structure while standardizing it
# test_image_flair = scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)

# print("Shape: ", test_image_flair.shape)
# print("Dtype: ", test_image_flair.dtype)
# print("Min: ", test_image_flair.min())
# print("Max: ", test_image_flair.max())

"""Let's rescale the rest of the images from patient 147"""

# # Rescale t1
# test_image_t1 = nib.load(test_PATH + "BraTS20_Training_147/BraTS20_Training_147_t1.nii").get_fdata()
# test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)

# # Rescale t1ce
# test_image_t1ce = nib.load(test_PATH + "BraTS20_Training_147/BraTS20_Training_147_t1ce.nii").get_fdata()
# test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)

# # Rescale t2
# test_image_t2 = nib.load(test_PATH + "BraTS20_Training_147/BraTS20_Training_147_t2.nii").get_fdata()
# test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)

# # Don't want to rescale the mask - let's explore it before doing anything to it
# # Mask (seg)
# test_image_seg = nib.load(test_PATH + "BraTS20_Training_147/BraTS20_Training_147_seg.nii").get_fdata()

"""#### View by slice
Now that we've rescaled, let's take a look at the images for a couple of different slices. Let's arbitrarily pick slice 95 and slice 105.
"""

# slice = 95

# print("Slice number: ", str(slice))

# # Let's also check the shapes of the images t1 and mask
# # Modality shape
# print("Modality: ", test_image_t1.shape)

# # Segmentation shape
# print("Segmentation: ", test_image_seg.shape)

# plt.figure(figsize=(12,8))

# # Plot T1
# plt.subplot(2,3,1)
# plt.imshow(test_image_t1[:,:,slice], cmap='gray')
# plt.title('T1')

# # Plot T1CE
# plt.subplot(2,3,2)
# plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')
# plt.title('T1CE')

# # Plot T2
# plt.subplot(2,3,3)
# plt.imshow(test_image_t2[:,:,slice], cmap='gray')
# plt.title('T2')

# # Plot FLAIR
# plt.subplot(2,3,4)
# plt.imshow(test_image_flair[:,:,slice], cmap='gray')
# plt.title('Flair')

# # Plot Mask (Seg)
# plt.subplot(2,3,5)
# plt.imshow(test_image_seg[:,:,slice])
# plt.title('Mask')

# plt.show()

# slice = 105

# print("Slice number: ", str(slice))

# # Let's also check the shapes of the images t1 and mask
# # Modality shape
# print("Modality: ", test_image_t1.shape)

# # Segmentation shape
# print("Segmentation: ", test_image_seg.shape)

# plt.figure(figsize=(12,8))

# # Plot T1
# plt.subplot(2,3,1)
# plt.imshow(test_image_t1[:,:,slice], cmap='gray')
# plt.title('T1')

# # Plot T1CE
# plt.subplot(2,3,2)
# plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')
# plt.title('T1CE')

# # Plot T2
# plt.subplot(2,3,3)
# plt.imshow(test_image_t2[:,:,slice], cmap='gray')
# plt.title('T2')

# # Plot FLAIR
# plt.subplot(2,3,4)
# plt.imshow(test_image_flair[:,:,slice], cmap='gray')
# plt.title('Flair')

# # Plot Mask (Seg)
# plt.subplot(2,3,5)
# plt.imshow(test_image_seg[:,:,slice])
# plt.title('Mask')

# plt.show()

"""#### View all slices
It's pretty easy to tell at this point that depending on the slice, the images will show different things. We can then take a look at all slices and see if each will give different information, important or not.
"""

# plt.figure(figsize=(10,10))
# plt.subplot(1,1,1)

# # Montage helps show all of the images
# plt.imshow(rotate(montage(test_image_t2[:,:,:]), 90, resize=True), cmap='gray')
# plt.show()

"""The sides of the image are dark, which means they don't contain much useful information. Let's remove 50 slices from each side to see if all important information is contained. Can adjust number of slices if needed."""

# plt.figure(figsize=(10,10))
# plt.subplot(1,1,1)

# # Let's remove 50 slices on each side since there's not much information from them
# plt.imshow(rotate(montage(test_image_t2[50:-50,:,:]), 90, resize=True), cmap='gray')
# plt.show()

"""#### Look at different viewpoints
Next, let's take a look at the different views of a single slice. We'll look at the transverse view (above), the frontal view, and the saggital view (side). <br> <br>
We do this because the different viewpoints can give us different understandings of the images.
"""

# slice = 95

# print("Slice: ", str(slice))

# plt.figure(figsize=(12,8))

# # Transverse view
# plt.subplot(1,3,1)
# plt.imshow(test_image_t2[:,:,slice], cmap='gray')
# plt.title("Transverse View")

# # Frontal view
# plt.subplot(1,3,2)
# plt.imshow(rotate(test_image_t2[:,slice,:], 90, resize=True), cmap='gray')
# plt.title("Frontal View")

# # Saggital view
# plt.subplot(1,3,3)
# plt.imshow(rotate(test_image_t2[slice,:,:], 90, resize=True), cmap='gray')
# plt.title("Saggital View")

# plt.show()

"""#### Segmented images
Now that we've viewed much of the data, let's focus on the segmented images.<br><br>
There are 4 important classes in the segmented images:
1. Not Tumor (class 0)
2. Non-Enhancing Tumor (class 1)
3. Edema (class 2)
4. Enhancing Tumor (class 4)<br>

We'll now take a look at each of them separately
"""

# slice = 95
# print("Slice: ", str(slice))

# # Lets define the classes and the coloring scheme
# cmap = matplotlib.colors.ListedColormap(['#500050', '#0096FF', '#00FF00', '#CCCC00'])
# norm = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)

# # Not Tumor (class 0)
# seg_0 = test_image_seg.copy()
# seg_0[seg_0 != 0] = np.nan

# # Non-Enhancing Tumor (class 1)
# seg_1 = test_image_seg.copy()
# seg_1[seg_1 != 1] = np.nan

# # Edema (class 2)
# seg_2 = test_image_seg.copy()
# seg_2[seg_2 != 2] = np.nan

# # Enhancing Tumor (class 4)
# seg_4 = test_image_seg.copy()
# seg_4[seg_4 != 4] = np.nan

# # Define class names for legend
# class_names = ['class 0', 'class 1', 'class 2', 'class 4']
# legend = [plt.Rectangle((0,0),1,1, color = cmap(i), label = class_names[i]) for i in range(len(class_names))]


# plt.figure(figsize=(20,20))

# plt.subplot(1,5,1)
# plt.imshow(test_image_seg[:,:,slice], cmap=cmap, norm=norm)
# plt.title('Full Mask')
# plt.legend(handles=legend, loc='lower left')

# plt.subplot(1,5,2)
# plt.imshow(seg_0[:,:,slice], cmap=cmap, norm=norm)
# plt.title('Seg 0')
# plt.legend(handles=legend, loc='lower left')

# plt.subplot(1,5,3)
# plt.imshow(seg_1[:,:,slice], cmap=cmap, norm=norm)
# plt.title('Seg 1')
# plt.legend(handles=legend, loc='lower left')

# plt.subplot(1,5,4)
# plt.imshow(seg_2[:,:,slice], cmap=cmap, norm=norm)
# plt.title('Seg 2')
# plt.legend(handles=legend, loc='lower left')

# plt.subplot(1,5,5)
# plt.imshow(seg_4[:,:,slice], cmap=cmap, norm=norm)
# plt.title('Seg 4')
# plt.legend(handles=legend, loc='lower left')

# plt.show()

"""## 5. Split the Data"""

import os
import glob
from sklearn.model_selection import train_test_split

# Define the base directory for the Task_01 Brain tumor dataset
base_dir = "/content/drive/MyDrive/BrainTumorSegmentation"
task01_dir = os.path.join(base_dir, "Task01_BrainTumour")

# Directories for images and labels
images_train_dir = os.path.join(task01_dir, "imagesTr")
labels_train_dir = os.path.join(task01_dir, "labelsTr")

# Gather all image and label files
train_image_files = sorted(glob.glob(os.path.join(images_train_dir, "*.nii.gz")))
train_label_files = sorted(glob.glob(os.path.join(labels_train_dir, "*.nii.gz")))

# Pair each image file with its corresponding label file
train_files = list(zip(train_image_files, train_label_files))

# Print the number of training examples
print(f"Total number of training examples: {len(train_files)}")

# Split the data into train/test and validation sets (20% validation)
train_test_files, val_files = train_test_split(train_files, test_size=0.2, random_state=42)

# Further split the train/test into train (80% train) and test (20% of train_test_files)
train_files, test_files = train_test_split(train_test_files, test_size=0.25, random_state=42)  # Adjusts for 15% of the original set

# Print the sizes of each dataset split
print(f"Training set size: {len(train_files)}")
print(f"Testing set size: {len(test_files)}")
print(f"Validation set size: {len(val_files)}")

# from google.colab import drive
# drive.mount('/content/drive')

"""Since we don't have a defined test set, we can split the validation set randomly to get a train/val/test split to about 65/20/15"""

# # Split the data into train/test and validation (20% validation)
# train_test_files, val_files = train_test_split(train_and_test_files,test_size=0.2)

# # Split the train/test into train (68% train) and test (12% test)
# train_files, test_files = train_test_split(train_test_files,test_size=0.15)

# Print data distribution (Train: 68%, Test: 12%, Val: 20%)
print(f"Train length: {len(train_files)}")
print(f"Validation length: {len(val_files)}")
print(f"Test length: {len(test_files)}")

plt.bar(["Train","Valid","Test"],
        [len(train_files), len(val_files), len(test_files)],
        align='center',
        color=[ 'green','red', 'blue'],
        label=["Train", "Valid", "Test"]
       )

plt.legend()

plt.ylabel('Number of Images')
plt.title('Data Distribution')

plt.show()

"""## 6. Define Transformations for the data

The following transformations are applied to the **training**, **validation**, and **test datasets** to preprocess MRI images and their corresponding segmentation masks. These steps ensure the images are ready for training the model and evaluating its performance.

---

<b>Common Transformations</b><br>
Both training and validation pipelines share a few common transformations.

**`LoadImaged(keys=["t1ce", "flair", "seg"])`**
- Loads the T1CE and FLAIR MRI modalities along with the segmentation labels (masks) from the file paths. This transformation ensures that all necessary image volumes and their corresponding masks are loaded correctly.

**`EnsureChannelFirstd(keys=["t1ce", "flair", "seg"])`**
- Ensures that the input data (both MRI images and masks) have channels as the first dimension (e.g., `[C, H, W]` format). This is important for neural networks that expect inputs in a specific order (e.g., channel-first format for image data).

**`ScaleIntensityd(keys=["t1ce", "flair"])`**
- This transformation normalizes the intensity values of the T1CE and FLAIR images. This ensures that the pixel values are scaled to a common range, which helps the model learn better from the data by reducing the effects of variations in image intensity.

**`Resized(keys=["t1ce", "flair", "seg"], spatial_size=common_size)`**
- Resizes the images and masks to a common spatial size of `(240, 240, 160)`. This ensures that all input data have the same dimensions, which is required for batch processing during model training.

**`ReassignClasses()`**
- Custom class that reassigns the segmentation labels. In the BraTS dataset, class `4` (GD-enhancing tumor) is reassigned to class `3` (to maintain label continuity), while classes `1` and `2` are left unchanged.

**`EnsureTyped(keys=["t1ce", "flair", "seg"])`**
- Ensures that the data is converted into PyTorch tensors, which is necessary for compatibility with the deep learning model during training and evaluation.

**`AsDiscreted(keys=["seg"], to_onehot=4)`**
- Converts the segmentation labels to one-hot encoded format with four classes. This is essential for multi-class segmentation tasks where each pixel is classified into one of several categories (e.g., necrotic core, edema, enhancing tumor).

---

<b>Additional Training-Specific Transformations</b>

For the **training dataset**, additional augmentations are applied to improve the model's generalization capabilities.

**`RandFlipd(keys=["t1ce", "flair", "seg"], spatial_axis=0, prob=0.5)`**
- Randomly flips the images and masks along the specified axis (e.g., left-right flipping). This helps introduce variability into the training data, which can prevent overfitting.

**`RandRotate90d(keys=["t1ce","flair", "seg"], prob=0.5)`**
- Randomly rotates the images and segmentation masks by 90 degrees. Like flipping, this adds variation to the data, improving the model's ability to generalize.

**`RandZoomd(keys=["t1ce","flair", "seg"], min_zoom=0.9, max_zoom=1.1, prob=0.5)`**
- Randomly zooms in or out of the images and masks within a specified range. This ensures that the model learns to handle different scales of tumors and other features in the images.

---

<b>Additional Notes</b><br>
- **Training Augmentations** like flipping, rotating, and zooming are applied only to the training data to increase its variability. This helps the model generalize better during inference.
- **Validation and Test Transformations** do not include random augmentations to ensure consistency during evaluation.

These transformations collectively ensure that the input data is well-prepared for training and validation, with variability in training data and consistency in validation/test data.
"""

common_size = (240, 240, 160)  # Ensure a common size for spatial dimensions

class ReassignClasses:
    def __call__(self, data):
        seg = data['seg']
        seg[seg == 4] = 3  # Reassign class 4 to class 3
        data['seg'] = seg
        return data

# Define transformations for training, validation, and test datasets
train_transforms = Compose(
    [
        LoadImaged(keys=["t1ce","flair", "seg"]),  # Load images from file paths
        EnsureChannelFirstd(keys=["t1ce","flair", "seg"]),  # Ensure channels are first
        ScaleIntensityd(keys=["t1ce","flair"]),  # Normalize intensity values
        Resized(keys=["t1ce", "flair", "seg"], spatial_size=common_size), # Ensure common size
        RandFlipd(keys=["t1ce","flair", "seg"], spatial_axis=0, prob=0.5),  # Random flip
        RandRotate90d(keys=["t1ce","flair", "seg"], prob=0.5),  # Random 90 degree rotation
        RandZoomd(keys=["t1ce","flair", "seg"], min_zoom=0.9, max_zoom=1.1, prob=0.5),  # Random zoom
        ReassignClasses(),
        AsDiscreted(keys=["seg"], to_onehot=4),
        EnsureTyped(keys=["t1ce","flair", "seg"]) # Ensure tensors
    ]
)

val_transforms = Compose(
    [
        LoadImaged(keys=["t1ce", "flair", "seg"]),  # Load images from file paths
        EnsureChannelFirstd(keys=["t1ce", "flair", "seg"]),  # Ensure channels are first
        ScaleIntensityd(keys=["t1ce", "flair"]),  # Normalize intensity values
        Resized(keys=["t1ce", "flair", "seg"], spatial_size=common_size),  # Resize images
        ReassignClasses(),  # Reassign segmentation labels
        EnsureTyped(keys=["t1ce", "flair", "seg"]),  # Ensure tensors
        AsDiscreted(keys=["seg"], to_onehot=4)  # One-hot encoding for segmentation
    ]
)

"""## 7. Create DataLoaders for split data"""

# Create MONAI datasets for training, new validation, and test sets
train_ds = Dataset(data=train_files, transform=train_transforms)
val_ds = Dataset(data=val_files, transform=val_transforms)
test_ds = Dataset(data=test_files, transform=val_transforms)  # Test set with no augmentation

# Create DataLoaders
train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)
val_loader = DataLoader(val_ds, batch_size=2, num_workers=0)
test_loader = DataLoader(test_ds, batch_size=2, num_workers=0)

# Print data loader sizes
print(f"Training DataLoader size: {len(train_loader)}")
print(f"Validation DataLoader size: {len(val_loader)}")
print(f"Test DataLoader size: {len(test_loader)}")

"""## 8. Create model, loss function, optimizer

We'll now be creating the <b>model</b>, <b>loss function</b>, and <b>optimizer</b> for our data. <br><br>
**`Model: UNet`**<br>
For this project, we are using <b>UNet</b>, a popular architecture in medical image segmentation tasks. UNet is specifically designed for tasks where the output needs to have the same spatial resolution as the input, like segmentation, where each pixel of the input corresponds to a class in the output. It uses an encoder-decoder structure with skip connections, which helps in capturing both high-level and low-level features efficiently, making it well-suited for segmenting complex structures like brain tumors from MRI scans.<br><br>
**`Loss Function: DiceLoss`**<br>
The <b>Dice Loss</b> is a great choice for image segmentation tasks. It directly optimizes the overlap between predicted and ground truth segmentations, making it particularly useful for medical segmentation where class imbalances (e.g., small tumor regions compared to larger healthy tissue) are common. Dice Loss works by maximizing the similarity between the predicted mask and the ground truth, focusing on the true positive overlap between them.<br><br>
**`Optimizer: Adam`**<br>
We are using the <b>Adam</b> optimizer for training. Adam is known for its efficiency in terms of both memory and computation, as well as for its ability to adapt the learning rate of each parameter based on the first and second moments of the gradients. This is particularly helpful in tasks like medical image segmentation, where models can require fine-tuning for complex data, such as MRI images, and Adam’s adaptive learning rate can make the training process more stable and faster compared to more basic optimizers like SGD.<br>
"""

max_epochs = 100
val_interval = 1
VAL_AMP = False

# Define the model
device = torch.device("cpu")
model = UNet(
    spatial_dims=3,           # 3D UNet for 3D images
    in_channels=2,            # Input channels (T1CE and FLAIR)
    out_channels=4,           # Output channels (for 4 classes)
    channels=(8, 16, 32, 64, 128),  # Number of filters in each level
    strides=(2, 2, 2, 2),     # Strides for downsampling in each level
    num_res_units=2,          # Use residual connections
    dropout=0.2               # Dropout probability
).to(device)

# Define loss function, optimizer, learning rate scheduler
loss_function = DiceLoss(
    smooth_nr = 1,
    smooth_dr = 1e-5,
    squared_pred = True,
    to_onehot_y = False,
    sigmoid = False
)
# SGD optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
# Step LR
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)

# Metrics and post-processing
dice_metric = DiceMetric(include_background = False, reduction = "mean")
dice_metric_batch = DiceMetric(include_background = False, reduction = "mean_batch")

post_trans = Compose([
    Activations(softmax=True),           # Apply softmax to get class probabilities
    # AsDiscrete(argmax=True)              # Convert probabilities to class labels using argmax
    AsDiscrete(threshold = 0.25)           # Lower threshold for classes that aren't as prevalent
])

# Define inference method
def inference(input):
    def _compute(input):
        return sliding_window_inference(
            inputs=input,
            roi_size=(240, 240, 160),
            sw_batch_size=1,
            predictor=model,
            overlap=0.5,
        )
    return _compute(input)

torch.backends.cudnn.benchmark = False

"""## 9. Train the data

The training loop is the core process where the model learns from the data. It runs for a specified number of **epochs**, where each epoch represents one complete pass through the entire training dataset.

During each epoch, the data is processed in smaller groups called **batches**. This is done to make training more memory efficient and to allow for more stable updates to the model parameters. The model processes each batch individually, calculates the loss, and updates its parameters.

**Epoch loop:**  
The outer loop runs for a predefined number of epochs. Each epoch consists of processing all the data in batches. Multiple epochs are used because the model needs to see the data many times to learn meaningful patterns. The loop begins with setting the model to training mode using `model.train()`, ensuring that all the necessary layers (like dropout) behave correctly during training.

**Batch processing:**  
Within each epoch, the dataset is processed in batches. A batch is a subset of the dataset, usually smaller than the entire dataset, that can fit into memory. Each batch is loaded from the `train_loader` and consists of MRI scans and their corresponding segmentation masks.

**Forward pass:**  
For each batch, the model makes predictions by performing a forward pass. This involves passing the batch of images through the layers of the model to get an output (in this case, a segmentation mask). The predicted output is compared to the true labels (segmentation masks) using a **loss function** (here, Dice Loss). The loss function measures how different the model’s predictions are from the ground truth.

**Backward pass (Backpropagation):**  
Once the loss is calculated, backpropagation is used to compute the gradients of the loss with respect to each of the model’s parameters. This step allows the model to understand how it should adjust its weights to minimize the error in future predictions. The `loss.backward()` call performs this gradient computation.

**Optimizer step:**  
After computing the gradients, the optimizer (Adam in this case) updates the model’s parameters based on these gradients. The `optimizer.step()` function performs this update, moving the model’s weights in the direction that reduces the loss.

**Epoch loss calculation:**  
During each epoch, the total loss is accumulated over all batches. After processing all the batches, the average loss for the epoch is calculated and stored. Monitoring the epoch loss helps in understanding whether the model is improving over time.

**Validation step:**  
After each epoch, the model is evaluated on a separate validation set. This step is crucial as it checks how well the model is generalizing to unseen data. The validation loop is similar to the training loop, but no gradients are calculated, and no weights are updated. The validation performance is used to track the model's improvement and prevent overfitting.

**Learning rate scheduling:**  
If a learning rate scheduler is used, the learning rate is adjusted after each epoch. This helps the model converge by starting with larger updates and gradually making smaller adjustments as training progresses.

**Saving the best model:**  
At the end of each epoch, the model's performance on the validation set is compared to previous epochs. If the model achieves a better validation performance (usually measured using a metric like the Dice score), the model’s parameters are saved. This ensures that the best version of the model, based on validation performance, is preserved for later use.
"""

best_metric = -1
best_metric_epoch = -1
best_metric_epoch_time = [[], [], []]
epoch_loss_val = []
metric_val = []
metric_val_tc = []
metric_val_wt = []
metric_val_et = []

total_start = time.time()
for epoch in range(max_epochs):
    epoch_start = time.time()
    print("-" * 10)
    print(f"epoch {epoch + 1}/{max_epochs}")

    model.train() # start training the model
    epoch_loss = 0
    step = 0

    # Iterate through batches
    for batch_data in train_loader:
        step_start = time.time()
        step += 1

        inputs = torch.cat([
                        batch_data["t1ce"].to(device),
                        batch_data["flair"].to(device)], dim=1)  # Concatenating along the channel axis
        labels = batch_data["seg"].to(device)  # Segmentation labels

        optimizer.zero_grad() # Reset gradients

        # Forward pass
        outputs = model(inputs)
        loss = loss_function(outputs, labels)

        # Backprop
        loss.backward()
        optimizer.step() # Update parameters

        # Accumulate total epoch loss
        epoch_loss += loss.item()

        # Update training progress
        print(
            f"{step}/{len(train_ds) // train_loader.batch_size}"
            f", train_loss: {loss.item():.4f}"
            f", step time: {(time.time() - step_start):.4f}"
        )

     # Update learning rate scheduler
    lr_scheduler.step()

    # Calc/store average loss for epoch
    epoch_loss /= step
    epoch_loss_val.append(epoch_loss)
    print(f"epoch {epoch + 1} average loss: {epoch_loss:.4f}")

    # Validation at intervals
    if (epoch + 1) % val_interval == 0:
        model.eval() # Evaluation mode
        with torch.no_grad(): # Don't want gradients when validating
            for val_data in val_loader:
                val_inputs = torch.cat([
                    val_data["t1ce"].to(device),
                    val_data["flair"].to(device)], dim=1)  # Concatenating along the channel axis
                val_labels = val_data["seg"].to(device)  # Segmentation labels

                # Make prediction
                val_outputs = inference(val_inputs)

                # Post-process outputs
                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]

                # Compute Dice metric for each batch
                dice_metric(y_pred = val_outputs, y = val_labels)
                dice_metric_batch(y_pred = val_outputs, y = val_labels)

            # Aggregate/store validation metrics
            metric = dice_metric.aggregate()
            curr_overall_dice = torch.mean(metric).item()
            metric_val.append(curr_overall_dice)

            # Aggregate metrics for this batch
            metric_batch = dice_metric_batch.aggregate()

            # Compute the mean of per-class Dice scores over the batch
            metric_val_tc = metric_batch[0].item()
            metric_val_wt = metric_batch[1].item()
            metric_val_et = metric_batch[2].item()

            curr_overall_dice_value = curr_overall_dice.item() if isinstance(curr_overall_dice, torch.Tensor) else curr_overall_dice
            metric_tc_value = metric_tc.item() if isinstance(metric_tc, torch.Tensor) else metric_tc
            metric_wt_value = metric_wt.item() if isinstance(metric_wt, torch.Tensor) else metric_wt
            metric_et_value = metric_et.item() if isinstance(metric_et, torch.Tensor) else metric_et

            metric_val_tc.append(metric_tc)
            metric_val_wt.append(metric_wt)
            metric_val_et.append(metric_et)

            # Reset metrics for next epoch
            dice_metric.reset()
            dice_metric_batch.reset()

            # Save model if current metric is the best
            if curr_overall_dice > best_metric:
                best_metric = curr_overall_dice
                best_metric_epoch = epoch + 1
                best_metric_epoch_time[0].append(best_metric)
                best_metric_epoch_time[1].append(best_metric_epoch)
                best_metric_epoch_time[2].append(time.time() - total_start)

                # Save model
                torch.save(
                    model.state_dict(),
                    os.path.join(root_dir, "best_metric_model.pth")
                )
                print("saved new best metric model")

            # Print best metrics
            print(
                f"current epoch: {epoch + 1} current mean dice: {curr_overall_dice_value:.4f}"
                f" tc: {metric_tc_value:.4f} wt: {metric_wt_value:.4f} et: {metric_et_value:.4f}"
                f"\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}"
            )

    # Time spent for each epoch
    print(f"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}")


# Save the lists as a pickle file
with open(os.path.join(root_dir, "training_metrics.pkl"), 'wb') as f:
    pickle.dump({
        'epoch_loss_val': epoch_loss_val,
        'metric_val': metric_val,
        'metric_val_tc': metric_val_tc,
        'metric_val_wt': metric_val_wt,
        'metric_val_et': metric_val_et,
    }, f)

print("Training metrics saved!")


# Calc total training time
total_time = time.time() - total_start
print(f"Total time: {total_time:.4f}")

"""#### Plot Loss and Metric from training"""

# Load the saved metrics
with open(os.path.join(root_dir, "training_metrics.pkl"), 'rb') as f:
    metrics = pickle.load(f)

# Now you can access the lists
epoch_loss_val = metrics['epoch_loss_val']
metric_val = metrics['metric_val']
metric_val_tc = metrics['metric_val_tc']
metric_val_wt = metrics['metric_val_wt']
metric_val_et = metrics['metric_val_et']

# Plot the variables for training evaluation
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.plot([i + 1 for i in range(len(epoch_loss_val))], epoch_loss_val, color='blue')
plt.title('Epoch Loss Values')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1,2,2)
plt.plot(val_interval * [i + 1 for i in range(len(epoch_loss_val))], metric_val, color='red')
plt.title('Validation Mean Dice')
plt.xlabel('Epoch')
plt.ylabel('Dice Value')

plt.figure(figsize=(18,12))
plt.subplot(1,3,1)
plt.plot(val_interval * [i + 1 for i in range(len(epoch_loss_val))], metric_val_tc, color='black')
plt.title('Validation Mean Dice TC')
plt.xlabel('Epoch')
plt.ylabel('Dice TC Value')

plt.subplot(1,3,2)
plt.plot(val_interval * [i + 1 for i in range(len(epoch_loss_val))], metric_val_wt, color='green')
plt.title('Validation Mean Dice WT')
plt.xlabel('Epoch')
plt.ylabel('Dice WT Value')

plt.subplot(1,3,3)
plt.plot(val_interval * [i + 1 for i in range(len(epoch_loss_val))], metric_val_et, color='purple')
plt.title('Validation Mean Dice ET')
plt.xlabel('Epoch')
plt.ylabel('Dice ET Value')

plt.tight_layout()
plt.show()

"""## 10. Evaluate the Model on Test Set

Now that we have trained our model, we will evaluate it on the test set, again using Dice as our metric
"""

# Load the best model
model.load_state_dict(torch.load(root_dir + "/best_metric_model.pth", map_location=device))

# Metrics for evaluation
dice_metric = DiceMetric(include_background = True, reduction = "mean")
dice_metric_batch = DiceMetric(include_background = True, reduction = "mean_batch")

# Post-processing: apply activation and thresholding for segmentation
post_trans = Compose([
    Activations(softmax=True),           # Apply softmax to get class probabilities
    # AsDiscrete(argmax=True)              # Convert probabilities to class labels using argmax
    AsDiscrete(threshold = 0.25)           # Lower threshold for classes that aren't as prevalent
])
model.eval()  # Set model to evaluation mode
dice_scores = []

with torch.no_grad():
    for idx, test_data in enumerate(test_loader):
        # Prepare test inputs by concatenating T1CE and FLAIR along the channel axis
        test_inputs = torch.cat([
            test_data['t1ce'].to(device),
            test_data['flair'].to(device)], dim=1)

        test_labels = test_data['seg'].to(device)  # Get segmentation labels

        # Forward pass through the model
        outputs = model(test_inputs)

        # Post-processing outputs
        outputs = [post_trans(i) for i in decollate_batch(outputs)]

        # Compute Dice metric for this batch
        dice_metric(y_pred=outputs, y=test_labels)

        # Get the Dice score for the batch and append to dice_scores list
        dice_score_batch = dice_metric.aggregate().mean().item()
        dice_scores.append(dice_score_batch)

        # Reset the dice metric for the next batch
        dice_metric.reset()

        print(f"Dice score for batch {idx + 1}: {dice_score_batch:.4f}")

# Calculate the final average Dice score across all batches
final_dice_score = sum(dice_scores) / len(dice_scores) if dice_scores else 0.0
print(f"Final Dice score on test set: {final_dice_score:.4f}")

"""### Visualize the model evaluation"""

def plot_seg(image_seg, slice_idx):
    # Lets define the classes and the coloring scheme
    cmap = matplotlib.colors.ListedColormap(['#500050', '#0096FF', '#00FF00', '#CCCC00'])
    norm = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)

    image_seg = image_seg.cpu().numpy()  # Convert to NumPy for visualization

    # Combine the channels into a full mask
    # Start with an empty mask (assume class 0, non-tumor by default)
    full_mask = np.zeros(image_seg[0].shape, dtype=np.int8)

    # Apply each class label to the appropriate voxels
    full_mask[image_seg[1] == 1] = 1  # Non-Enhancing Tumor (class 1)
    full_mask[image_seg[2] == 2] = 2  # Edema (class 2)
    full_mask[image_seg[3] == 4] = 4  # Enhancing Tumor (class 4)

    # Not Tumor (class 0)
    seg_0 = np.copy(image_seg[0])
    seg_0[seg_0 != 0] = np.nan

    # Non-Enhancing Tumor (class 1)
    seg_1 = np.copy(image_seg[1])
    seg_1[seg_1 != 1] = np.nan

    # Edema (class 2)
    seg_2 = np.copy(image_seg[2])
    seg_2[seg_2 != 2] = np.nan

    # Enhancing Tumor (class 4)
    seg_4 = np.copy(image_seg[3])
    seg_4[seg_4 != 3] = np.nan

    # Define class names for legend
    class_names = ['class 0', 'class 1', 'class 2', 'class 4']
    legend = [plt.Rectangle((0,0),1,1, color = cmap(i), label = class_names[i]) for i in range(len(class_names))]


    plt.figure(figsize=(20,20))

    plt.subplot(1,5,1)
    plt.imshow(full_mask[:,:,slice_idx], cmap=cmap, norm=norm)
    plt.title('Full Mask')
    plt.legend(handles=legend, loc='lower left')

    plt.subplot(1,5,2)
    plt.imshow(seg_0[:,:,slice_idx], cmap=cmap, norm=norm)
    plt.title('Seg 0')
    plt.legend(handles=legend, loc='lower left')

    plt.subplot(1,5,3)
    plt.imshow(seg_1[:,:,slice_idx], cmap=cmap, norm=norm)
    plt.title('Seg 1')
    plt.legend(handles=legend, loc='lower left')

    plt.subplot(1,5,4)
    plt.imshow(seg_2[:,:,slice_idx], cmap=cmap, norm=norm)
    plt.title('Seg 2')
    plt.legend(handles=legend, loc='lower left')

    plt.subplot(1,5,5)
    plt.imshow(seg_4[:,:,slice_idx], cmap=cmap, norm=norm)
    plt.title('Seg 4')
    plt.legend(handles=legend, loc='lower left')

    plt.show()

# After each batch or at the end of testing
post_trans = Compose([
    Activations(softmax=True),           # Apply softmax to get class probabilities
    # AsDiscrete(argmax=True)              # Convert probabilities to class labels using argmax
    AsDiscrete(threshold = 0.25)           # Lower threshold for classes that aren't as prevalent
])
for idx, test_data in enumerate(test_loader):
    print(f"Input and output for data point {idx + 1}/{len(test_loader)}")
    test_inputs = torch.cat([test_data['t1ce'].to(device), test_data['flair'].to(device)], dim=1)
    test_labels = test_data['seg'].to(device)
    outputs = model(test_inputs)
    outputs = [post_trans(i) for i in decollate_batch(outputs)]

    # print(f"Inputs: {test_labels}")
    # print(f"Outputs: {outputs}")
    # print(f"Input label shape: {test_labels.shape}")
    # print(f"Output shape (after decollate): {outputs[0].shape}")

    plot_seg(test_labels[0], 80)  # Ground truth segmentation
    plot_seg(outputs[0], 80)  # Predicted segmentation

    #plot_segmentation_results(test_inputs, test_labels, outputs, idx)

"""## Future Work

1. **Improving Class Balance**:
   - **Loss Function Tuning**: Experiment with weighted loss functions (e.g., focal loss) to handle class imbalance and improve segmentation accuracy for underrepresented tumor regions.
   - **Data Augmentation**: Implement targeted augmentation strategies for minority classes to create more representative training data.
<br><br>

2. **Advanced Model Architectures**:
   - **Attention Mechanisms**: Integrate attention modules (e.g., Attention U-Net) to help the model focus on important regions and improve segmentation boundaries.
   - **Ensemble Models**: Combine predictions from multiple models to improve robustness and handle variability across patients.
<br><br>

3. **Transfer Learning**:
   - **Pre-trained Models**: Explore transfer learning using pre-trained models on other medical imaging tasks to improve initial weights and reduce training time.
<br><br>

4. **Post-processing Refinement**:
   - **Threshold Optimization**: Experiment with adaptive thresholding techniques for specific classes to improve segmentation accuracy.
   - **Morphological Operations**: Apply morphological operations (e.g., smoothing, erosion) on the output masks to improve boundary sharpness and remove noise.
<br><br>

5. **Validation on Additional Datasets**:
   - **Generalizability Testing**: Evaluate the model on additional datasets to assess generalizability and improve robustness.
   - **Cross-Validation**: Conduct cross-validation with different dataset splits to better understand model consistency and prevent overfitting.
<br><br>

6. **Deployment for Clinical Use**:
   - **Real-time Performance**: Optimize the model for real-time inference, including hardware acceleration (e.g., GPU optimizations) and memory management.
   - **User-friendly Interface**: Develop a simple interface for clinicians to use the model outputs in decision-making, with options for viewing and editing segmentation masks.
<br><br>

7. **Exploring Other Modalities**:
   - **Multimodal MRI**: Integrate additional MRI sequences (such as DWI or ADC maps) to provide richer input features.
   - **Cross-modal Fusion**: Investigate models capable of fusing information across different imaging modalities to improve segmentation accuracy.
"""